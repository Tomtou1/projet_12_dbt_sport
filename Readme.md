# üèÉ‚Äç‚ôÇÔ∏è Plateforme d'Analyse des Activit√©s Sportives

## üìã Description

Ce projet est une **plateforme moderne de data engineering** con√ßue pour suivre, analyser et r√©compenser les activit√©s sportives des salari√©s au sein d'une organisation. Construite avec un accent sur la **qualit√© des donn√©es**, la **confidentialit√©** et la **scalabilit√©**, elle impl√©mente un pipeline complet ELT (Extract, Load, Transform) en utilisant des outils standards de l'industrie et les meilleures pratiques.

### Fonctionnalit√©s Principales

- **üìä Suivi des Activit√©s Sportives des Salari√©s** : Collecte et stockage automatis√©s des activit√©s sportives incluant running, natation, cyclisme, randonn√©e et tennis
- **üè¢ Int√©gration des Donn√©es RH** : Gestion compl√®te des informations des salari√©s incluant les business units, salaires et d√©tails de trajets domicile-travail
- **üí∞ Calcul des Primes de Mobilit√©** : Validation intelligente des moyens de transport et calcul automatique des primes
- **üìà Analytics en Temps R√©el** : Tableaux de bord de business intelligence propuls√©s par Apache Superset
- **üí¨ Int√©gration Slack** : Notifications automatis√©es des activit√©s sportives quotidiennes
- **üîÑ Traitement Incr√©mental des Donn√©es** : Mises √† jour quotidiennes efficaces utilisant la strat√©gie de mat√©rialisation incr√©mentale de dbt

### Stack Technologique

- **Base de donn√©es** : PostgreSQL 16 avec Write-Ahead Logging (WAL) pour la durabilit√© des donn√©es
- **Transformation de donn√©es** : dbt (data build tool) avec architecture m√©daillon (couches Bronze/Silver/Gold)
- **Orchestration** : Docker Compose pour l'orchestration des conteneurs
- **BI/Visualisation** : Apache Superset pour les tableaux de bord et analytics
- **Qualit√© des donn√©es** : Tests dbt pour la validation des sch√©mas et l'int√©grit√© des donn√©es
- **Automatisation** : Ex√©cution quotidienne du pipeline via cron
- **Communication** : Webhooks Slack pour les notifications d'activit√©s

---

## üöÄ Performance & Scalabilit√©

### Optimisations de Performance

1. **Traitement Incr√©mental**
   - Les mod√®les incr√©mentaux dbt ne traitent que les donn√©es nouvelles/modifi√©es
   - R√©duit le temps de traitement d'heures √† minutes pour les grands volumes de donn√©es
   - Exemple : `silver_sport_activities_history` utilise `unique_key='id_activity'` pour des mises √† jour efficaces

2. **Strat√©gie de Vues Mat√©rialis√©es**
   - Couche Silver : Tables mat√©rialis√©es pour donn√©es nettoy√©es et d√©dupliqu√©es
   - Couche Gold : Tables agr√©g√©es pr√™tes pour le business
   - Minimise le temps d'ex√©cution des requ√™tes pour les analytics utilisateurs

3. **Indexation de la Base de Donn√©es**
   - Cl√©s primaires sur toutes les tables (`id_activity`, `id_salarie`, `id_duo`)
   - Optimis√© pour les op√©rations JOIN entre activit√©s, donn√©es RH et enregistrements sportifs

4. **Pooling de Connexions**
   - PostgreSQL configur√© avec health checks et logique de retry
   - Volumes persistants (`postgres_data`) pr√©viennent la perte de donn√©es et r√©duisent le temps de d√©marrage

### Caract√©ristiques de Scalabilit√©

#### Scalabilit√© Horizontale
- **Architecture Conteneuris√©e** : Chaque service (PostgreSQL, dbt, Superset, Slack) s'ex√©cute dans des conteneurs isol√©s
- **Pattern Microservices** : Mise √† l'√©chelle ind√©pendante des producteurs, transformateurs et consommateurs de donn√©es
- **Isolation R√©seau** : Bridge personnalis√© `sport_network` pour une communication inter-services efficace

#### Scalabilit√© Verticale
- **Configuration PostgreSQL WAL** : `wal_level=logical` active le Change Data Capture (CDC) pour de futures architectures √©v√©nementielles
- **Configuration des Threads dbt** : Traitement parall√®le ajustable (actuellement 1 thread, peut monter √† N threads)
- **Gestion des Volumes** : Les volumes persistants s√©parent le calcul du stockage

#### Gestion du Volume de Donn√©es
- **Capacit√© Actuelle** : G√®re 100+ activit√©s pour 2025 + 25+ pour 2026 (125 enregistrements)
- **Capacit√© Projet√©e** : L'architecture supporte des millions d'enregistrements gr√¢ce √† :
  - Capacit√© de partitionnement (natif PostgreSQL)
  - Traitement incr√©mental (uniquement nouvelles donn√©es)
  - Agr√©gation efficace dans la couche gold

#### Extensibilit√©
- **Nouvelles Sources de Donn√©es** : Int√©gration facile via de nouvelles d√©finitions de sources dbt
- **M√©triques Additionnelles** : La couche gold modulaire permet d'ajouter de nouveaux KPIs sans perturber les pipelines existants
- **Pr√™t Multi-tenant** : La s√©paration par business unit (`bu_salarie`) permet des analytics par d√©partement

---

## üîí S√©curit√© & Gestion des Mots de Passe

### Protection des Mots de Passe

1. **Variables d'Environnement**
   - Tous les identifiants stock√©s dans un fichier `.env` (exclu du contr√¥le de version via `.gitignore`)
   - Docker Compose utilise les variables `${POSTGRES_USER}`, `${POSTGRES_PASSWORD}`, `${POSTGRES_DB}`
   - Les profils dbt utilisent la fonction Jinja `env_var()` pour l'injection des identifiants au runtime

2. **Aucun Secret en Dur**
   - Identifiants de base de donn√©es : Variables d'environnement uniquement
   - Webhook Slack : Stock√© comme variable d'environnement `${SLACK_WEBHOOK_URL}`
   - Cl√© secr√®te Superset : `${SUPERSET_SECRET_KEY}` pour le chiffrement des sessions
   - Cl√© API Google Maps : `os.getenv("GCP_key")` dans les scripts Python

3. **Isolation des Conteneurs**
   - Les services communiquent via le r√©seau Docker interne (`sport_network`)
   - PostgreSQL non expos√© √† l'h√¥te (port interne 5432, externe 5433 pour acc√®s admin uniquement)
   - Mot de passe admin Superset d√©fini programmatiquement, non stock√© dans le code

4. **Contr√¥le d'Acc√®s**
   - Identifiants utilisateur de base de donn√©es configur√©s par environnement
   - Contr√¥le d'acc√®s bas√© sur les r√¥les Superset (RBAC)
   - Les scripts Python utilisent la biblioth√®que `dotenv` pour le chargement s√©curis√© des identifiants

### Bonnes Pratiques de S√©curit√©

- **Segmentation R√©seau** : Le r√©seau bridge personnalis√© emp√™che l'acc√®s externe non autoris√©
- **Health Checks** : S'assure que les services sont pr√™ts avant le d√©marrage des services d√©pendants
- **S√©curit√© des Images** : Utilise des images officielles (`debezium/postgres:16`, `apache/superset:latest-dev`)
- **Permissions des Volumes** : Les volumes de donn√©es PostgreSQL appartiennent √† l'utilisateur du conteneur

---

## üá´üá∑ Conformit√© RGPD

### Protection des Donn√©es Personnelles

1. **Minimisation des Donn√©es**
   - **Les adresses domicile sont anonymis√©es** : La fonction `clean_adresses()` supprime les adresses compl√®tes apr√®s calcul de distance
   - Seule la distance au bureau (`distance_to_office_km`) est conserv√©e, pas la localisation pr√©cise
   - Les salaires sont stock√©s comme agr√©gats pour les calculs de primes, non expos√©s dans les tableaux de bord

2. **Pseudonymisation des Noms**
   - Les noms des salari√©s utilisent un algorithme de divulgation progressive dans `silver_rh_info.sql` :
     - Pr√©nom uniquement si unique
     - Pr√©nom + 1 lettre du nom si n√©cessaire
     - Ajout progressif de caract√®res du nom jusqu'√† unicit√©
   - R√©duit l'identifiabilit√© tout en maintenant l'utilit√© des donn√©es

3. **Limitation de la Finalit√©**
   - **Activit√©s sportives** : Utilis√©es uniquement pour le suivi du programme bien-√™tre et les primes
   - **Donn√©es RH** : Limit√©es au salaire, BU, type de contrat et informations de trajet
   - **Donn√©es de mobilit√©** : Uniquement pour la validation de la prime de transport

4. **Conservation des Donn√©es**
   - Activit√©s historiques stock√©es ind√©finiment pour l'analyse des tendances (envisager d'impl√©menter une politique de r√©tention)
   - Les mod√®les incr√©mentaux maintiennent uniquement les fen√™tres temporelles pertinentes (ex: 12 derniers mois pour les analytics BU)

5. **Droit d'Acc√®s & Portabilit√©**
   - Tables PostgreSQL facilement interrogeables par `id_salarie`
   - Les tableaux de bord Superset fournissent un acc√®s en self-service aux donn√©es d'activit√© personnelles
   - Donn√©es exportables via requ√™tes SQL ou fonctionnalit√© d'export Superset

6. **Qualit√© & Exactitude des Donn√©es**
   - Les tests dbt assurent l'int√©grit√© des donn√©es (`not_null`, `unique`, `accepted_values`)
   - Validation des moyens de transport (`gold_incorrect_moyen_deplacement`) pour l'exactitude
   - Les salari√©s peuvent v√©rifier et corriger leurs informations via les syst√®mes RH

7. **S√©curit√© du Traitement**
   - Connexions chiffr√©es (possibilit√© d'activer SSL/TLS pour PostgreSQL)
   - Logs d'acc√®s via les capacit√©s d'audit PostgreSQL
   - L'environnement conteneuris√© r√©duit la surface d'attaque

### Fonctionnalit√©s Conformes RGPD

‚úÖ **Consentement** : Les salari√©s adh√®rent volontairement au programme de suivi sportif  
‚úÖ **Transparence** : Utilisation claire des donn√©es pour le bien-√™tre et les primes de mobilit√©  
‚úÖ **Droits des Personnes Concern√©es** : Capacit√©s de consultation et d'export  
‚úÖ **Privacy by Design** : Pseudonymisation et minimisation des donn√©es int√©gr√©es  
‚úÖ **Responsabilit√©** : Tra√ßabilit√© de la lign√©e dbt (`manifest.json`) pour la provenance des donn√©es  

### Am√©liorations Recommand√©es pour une Conformit√© RGPD Compl√®te

- [ ] Impl√©menter une politique de r√©tention des donn√©es (ex: supprimer les activit√©s de plus de 7 ans)
- [ ] Ajouter la journalisation d'audit pour tous les acc√®s aux donn√©es
- [ ] Cr√©er un syst√®me de gestion du consentement des salari√©s
- [ ] Documenter les accords de traitement des donn√©es (DPA) avec les tiers (API Google Maps, Slack)
- [ ] Impl√©menter un workflow automatis√© de suppression pour le "droit √† l'oubli"
- [ ] Activer le chiffrement SSL/TLS PostgreSQL pour les donn√©es en transit
- [ ] Ajouter le masquage des donn√©es pour les visualiseurs Superset sans permissions RH

---

## üèóÔ∏è Aper√ßu de l'Architecture

### Architecture M√©daillon (Bronze ‚Üí Silver ‚Üí Gold)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Couche Bronze  ‚îÇ  Sources de donn√©es brutes
‚îÇ  (PostgreSQL)   ‚îÇ  - sport_activities_history
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - sport_enterprise
         ‚îÇ           - rh_info
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Couche Silver  ‚îÇ  Donn√©es nettoy√©es & d√©dupliqu√©es
‚îÇ (mod√®les dbt)   ‚îÇ  - silver_sport_activities_history
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - silver_rh_info
         ‚îÇ           - silver_sport_enterprise
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Couche Gold    ‚îÇ  Agr√©gats pr√™ts pour le business
‚îÇ (mod√®les dbt)   ‚îÇ  - gold_sport_activities_per_bu
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - gold_sport_activities_prime_last_year
                     - gold_slack_message_activities
                     - gold_potential_moyen_deplac_bonus
                     - gold_incorrect_moyen_deplacement
```

### Flux d'Ex√©cution du Pipeline

1. **Extraction de Donn√©es** (`producer_history_strava`)
   - Charge les donn√©es RH depuis Excel (`Donnees_RH.xlsx`)
   - Charge les pr√©f√©rences sportives depuis Excel (`Donnees_Sportive.xlsx`)
   - G√©n√®re les activit√©s historiques (100 pour 2025, 25 pour 2026)
   - Calcule les distances au bureau via l'API Google Maps (simul√©)
   - Anonymise les adresses domicile

2. **Transformation de Donn√©es** (`elt_dbt`)
   - Ex√©cute les mod√®les dbt dans l'ordre de d√©pendance
   - Applique la logique incr√©mentale pour des mises √† jour efficaces
   - Ex√©cute les tests de qualit√© des donn√©es
   - Cr√©e les tables silver et gold

3. **Notification** (`slack_sender`)
   - Lit la table `gold_slack_message_activities`
   - Envoie des messages format√©s vers le webhook Slack
   - Inclut emojis, dur√©e et distance

4. **Visualisation** (`superset`)
   - Se connecte √† la couche gold PostgreSQL
   - Importe les tableaux de bord pr√©-configur√©s depuis `export.zip`
   - Sert les analytics sur le port 8088

### Automatisation Quotidienne

```bash
# cron/daily_pipeline.sh
docker compose up --force-recreate --no-deps \
  producer_history_strava dbt strava_sender
```

Planifi√© via cron job pour ex√©cuter les mises √† jour incr√©mentales quotidiennes.

---

## üõ†Ô∏è Installation & Configuration

### Pr√©requis

- Docker & Docker Compose
- Fichier `.env` avec les identifiants requis :

```bash
# Base de donn√©es
POSTGRES_USER=postgresuser
POSTGRES_PASSWORD=postgrespw
POSTGRES_DB=sport_activities_db

# Superset
SUPERSET_SECRET_KEY=votre_cle_secrete_ici

# Slack
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/VOTRE/WEBHOOK/URL

# API Google Maps (optionnel)
GCP_key=votre_cle_api_google_maps
```

### D√©marrage Rapide

```bash
# Rendre le script pipeline ex√©cutable
chmod +x cron/daily_pipeline.sh

# Lancer la stack compl√®te
docker compose up -d

# Acc√©der √† Superset
open http://localhost:8088
# Login: admin / admin

# Voir les logs
docker compose logs -f dbt
```

### Ex√©cution Manuelle de dbt

```bash
# Ex√©cuter les transformations
docker exec -it elt_dbt dbt run

# Ex√©cuter les tests
docker exec -it elt_dbt dbt test

# G√©n√©rer la documentation
docker exec -it elt_dbt dbt docs generate
docker exec -it elt_dbt dbt docs serve
```

---

## üìä Mod√®les de Donn√©es

### Tables de la Couche Gold

| Table | Description | Mat√©rialisation | Cl√© Incr√©mentale |
|-------|-------------|-----------------|------------------|
| `gold_sport_activities_per_bu` | Activit√©s par BU (12 derniers mois) | Incr√©mentale | `bu_salarie` |
| `gold_sport_activities_prime_last_year` | Salari√©s √©ligibles √† la prime 2025 (>3 activit√©s) | Incr√©mentale | `id_salarie` |
| `gold_slack_message_activities` | Activit√©s du jour pour notification Slack | Incr√©mentale | `[id_salarie, activity_date]` |
| `gold_potential_moyen_deplac_bonus` | Salari√©s √©ligibles √† la prime de mobilit√© | Table | - |
| `gold_incorrect_moyen_deplacement` | D√©clarations de transport invalides | Table | - |
| `gold_rh_info` | Donn√©es RH ma√Ætres avec noms pseudonymis√©s | Table | - |

### Logique Business Cl√©

- **√âligibilit√© Prime de Mobilit√©** : 
  - Le moyen de transport ne doit pas √™tre marche/v√©lo (d√©j√† subventionn√©s)
  - Distance au bureau < 25km
  - Prime = 5% du salaire brut
  
- **√âligibilit√© Prime Sportive** :
  - >3 activit√©s en 2025
  - Plusieurs types de sports pratiqu√©s

- **Pseudonymisation des Noms** :
  - L'algorithme de divulgation progressive assure l'unicit√© avec un minimum de donn√©es personnelles

---

## üìà Monitoring & Logs

- **Logs dbt** : `dbt/logs/dbt.log`
- **Logs pipeline** : `cron/logs/pipeline.log`
- **Logs conteneurs** : `docker compose logs <nom_service>`
- **Artefacts dbt** : `dbt/target/` (manifest, r√©sultats d'ex√©cution, SQL compil√©)

---

## üîÆ Prochaines √âtapes & Feuille de Route

### Am√©liorations Court Terme (3-6 mois)

1. **Architecture de Streaming Temps R√©el**
   - Impl√©menter Apache Kafka pour l'ingestion √©v√©nementielle des activit√©s
   - Utiliser le connecteur CDC Debezium (d√©j√† compatible avec l'image `debezium/postgres`)
   - Activer les notifications Slack en temps r√©el au lieu du batch quotidien

2. **Analytics Avanc√©es**
   - Mod√®le ML pour la d√©tection de fraude aux activit√©s (ex: distances/temps impossibles)
   - Analytics pr√©dictives pour les tendances de bien-√™tre des salari√©s
   - Analyse de cohorte par BU, type de contrat et moyen de transport

3. **Am√©liorations Qualit√© des Donn√©es**
   - Impl√©menter dbt-expectations pour des tests statistiques avanc√©s
   - Ajouter le profilage de donn√©es avec Great Expectations
   - Cr√©er un syst√®me d'alertes pour les tests dbt √©chou√©s (int√©gration Slack/email)

4. **Interface Utilisateur**
   - Construire un portail self-service pour les salari√©s (Flask/Streamlit)
   - Application mobile pour la journalisation des activit√©s (API REST avec FastAPI)
   - Classements en temps r√©el et gamification

### Objectifs Moyen Terme (6-12 mois)

5. **Int√©gration Multi-sources**
   - Int√©gration directe API Strava (remplacer les donn√©es simul√©es)
   - Connecteurs Garmin, Fitbit, Apple Health
   - Authentification OAuth2 pour les appareils des salari√©s

6. **Conformit√© RGPD Renforc√©e**
   - Politiques de r√©tention des donn√©es automatis√©es (partitionnement + suppression planifi√©e)
   - Tableau de bord de gestion du consentement
   - Pipeline d'anonymisation des donn√©es pour les datasets d'analytics
   - Piste d'audit compl√®te avec logs immuables

7. **Infrastructure as Code**
   - Terraform pour le d√©ploiement cloud (AWS RDS, ECS, S3)
   - Migration Kubernetes pour la scalabilit√© en production
   - Pipeline CI/CD (GitHub Actions) pour les tests de mod√®les dbt

8. **Optimisation des Performances**
   - Optimisation des requ√™tes PostgreSQL (EXPLAIN ANALYZE)
   - Ex√©cution parall√®le dbt (augmenter les threads)
   - Strat√©gie de rafra√Æchissement des vues mat√©rialis√©es
   - Couche de cache (Redis) pour les requ√™tes Superset

### Vision Long Terme (12+ mois)

9. **Fonctionnalit√©s Entreprise**
   - Architecture multi-tenant pour plusieurs entreprises
   - Tableaux de bord Superset en marque blanche
   - Monitoring SLA personnalis√© et alertes
   - Reprise apr√®s sinistre et configuration haute disponibilit√©

10. **Capacit√©s IA/ML**
    - Recommandations d'activit√©s personnalis√©es
    - Pr√©diction du d√©sengagement du programme bien-√™tre
    - D√©tection d'anomalies pour les patterns d'activit√©s suspects
    - Interface de requ√™te en langage naturel (analytics propuls√©es par LLM)

11. **Conformit√© & Gouvernance**
    - Certification de s√©curit√© ISO 27001
    - Conformit√© SOC 2 pour l'offre SaaS
    - Visualisation de la lign√©e des donn√©es (DAG dbt + Apache Atlas)
    - Reporting de conformit√© automatis√©

12. **Expansion de l'√âcosyst√®me**
    - API publique pour int√©grations tierces
    - Webhooks pour notifications personnalis√©es
    - Architecture de plugins pour types de sports personnalis√©s
    - Marketplace pour templates d'analytics

---

## ü§ù Contribuer

1. Forker le d√©p√¥t
2. Cr√©er une branche de fonctionnalit√© (`git checkout -b feature/fonctionnalite-incroyable`)
3. Commiter les changements (`git commit -m 'Ajout fonctionnalit√© incroyable'`)
4. Pousser vers la branche (`git push origin feature/fonctionnalite-incroyable`)
5. Ouvrir une Pull Request

---

## üìÑ Licence

Ce projet est d√©velopp√© dans le cadre du cursus Data Engineering d'OpenClassrooms.

---

## üë• Support & Contact

Pour toute question ou probl√®me :
- Cr√©er une issue sur GitHub
- Contact : thomas.leroy@example.com

---

**‚ö†Ô∏è Note Importante de Configuration :**
```bash
chmod +x cron/daily_pipeline.sh
```
Cette commande doit √™tre ex√©cut√©e avant de lancer le pipeline pour assurer les permissions appropri√©es.

---

*Derni√®re mise √† jour : F√©vrier 2026*
